{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Practical Test\n",
    "\n",
    "Â© Explore Data Science Academy\n",
    "\n",
    "The NLP practical test will take place within this Jupyter notebook. Each question will require you to write a function which will return the answer. This notebook will be graded automatically, so it is important that the names of any existing variables and functions are left unchanged.\n",
    "\n",
    "A shell function with the correct name for each question has already been defined for you. You will simply need to fill in the necessary code inside the function, as directed by the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honour Code\n",
    "\n",
    "I **Bruce**, **Kgarimetsa**, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the EDSA honour code (https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries and Read In the Data\n",
    "\n",
    "Do not modify or remove any of the code in these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\f5172993\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\f5172993\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\f5172993\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import TreebankWordTokenizer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import urllib\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's Adventures in Wonderland\n",
      "\n",
      "                ALICE'S ADVENTURES IN WONDERLAND\n",
      "\n",
      "                          Lewis Carroll\n",
      "\n",
      "               THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            CHAPTER I\n",
      "\n",
      "                      Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "  Alice was beginning to get very tired of sitting by her sister\n",
      "on the bank, and of having nothing to do:  once or twice she had\n",
      "peeped into the book her sister was reading, but it had no\n",
      "pictures or conversations in it, `and what is the use of a book,'\n",
      "thought Alice `without pictures or conversation?'\n",
      "\n",
      "  So she was considering in her own mind (as well as she could,\n",
      "for the hot day made her feel very sleepy and stupid), whether\n",
      "the pleasure of making a daisy-chain would be worth the trouble\n",
      "of getting up and picking the daisies, when suddenly a White\n",
      "Rabbit with pink eyes ran close by her.\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# read in the data\n",
    "def print_some_url():\n",
    "    with urllib.request.urlopen('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint//alice_in_wonderland.txt') as f:\n",
    "        return f.read().decode('ISO-8859-1')\n",
    "\n",
    "data = print_some_url()\n",
    "print(data[:863])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to lowercase and remove punctuation  \n",
    "\n",
    "Do not change or remove any of the code in these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    words = words.lower()\n",
    "    return ''.join([x for x in words if x not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_punctuation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a bag of words and assigning our stemmer and lemmatizer\n",
    "\n",
    "Pay special attention to what these functions return and how the subsequent texts and lists look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stemmer function\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# tokenise data\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "tokens = tokeniser.tokenize(data)\n",
    "\n",
    "# define lemmatiser\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# bag of words\n",
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "# remove stopwords\n",
    "tokens_less_stopwords = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# create bag of words\n",
    "bag_of_words = bag_of_words_count(tokens_less_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the stemmer and lemmatizer functions (defined in the cells above) from the relevant library to find the stem and lemma of the nth word in the token list.\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Should take a `list` as input and return a  `dict` type as output.\n",
    "* The dictionary should have the keys **'original',  'stem' and 'lemma'** with the corresponding values being the nth word transformed in that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "def find_roots(token_list, n):\n",
    "    # your code here\n",
    "    if n<0 or n>=len(token_list):\n",
    "        return {'original': none, 'stem': none, 'lemma': none}\n",
    "    \n",
    "    else:\n",
    "        word = token_list[n-1]\n",
    "        stem = stemmer.stem(word)\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "    \n",
    "    return {'original': word, 'stem': stem, 'lemma': lemma}\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'daisies', 'stem': 'daisi', 'lemma': 'daisy'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_roots(tokens, 120) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_roots(tokens, 120) == {'original': 'daisies', 'stem': 'daisi', 'lemma': 'daisy'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected Outputs:**_\n",
    "```python\n",
    "find_roots(tokens, 120) == \n",
    "{'original': 'daisies', \n",
    "'stem': 'daisi', \n",
    "'lemma': 'daisy'}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many stopwords are in the text in total?   \n",
    "\n",
    "_Hint_ : you can use the nltk stopwords dictionary \n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Function should take a `list` as input \n",
    "* The number of stopwords should be returned as an `int` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def count_stopwords(token_list):\n",
    "    # your code here\n",
    "    stop_words = [i for i in token_list if i in stopwords.words('english')]\n",
    "    num_stopwords = len(stop_words)\n",
    "    return num_stopwords\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13774"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_stopwords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_stopwords(tokens) == 13774"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected output:**_\n",
    "\n",
    "```python\n",
    "count_stopwords(tokens) == 13774\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "How many **unique** words are in the text?\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Function should take a `list` as input and return an `int` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def unique_words(token_list):\n",
    "    # your code here\n",
    "    unique_words_list = set(token_list)\n",
    "    num_unique_words = len(unique_words_list)\n",
    "    return num_unique_words\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words(tokens) == 2749"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected output:**_\n",
    "\n",
    "```python\n",
    "unique_words(tokens) == 2749\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the kth most frequently occuring word in the bag of words?\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Function should take a `dict` and an `int` k as input\n",
    "* Function should return the kth most common word as a `str`\n",
    "\n",
    "_Hint : bag_of_words already does not include stopwords_\n",
    "\n",
    "Example: \n",
    "```python\n",
    "most_common_word(bag = {'apple': 30, 'orange': 12, 'pear': 50, 'banana': 12}, 2)\n",
    "\n",
    ">>> 'apple'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def most_common_word(bag, k):\n",
    "    # your code here\n",
    "    sorted_dict = sorted(bag, key = lambda x: bag[x], reverse=True)\n",
    "    \n",
    "    if k <= len(bag):\n",
    "        kth_word = sorted_dict[k-1]\n",
    "        \n",
    "    return kth_word\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'little'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_word(bag_of_words, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_word(bag_of_words, 3) == 'little'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected output:**_\n",
    "\n",
    "```python\n",
    "most_common_word(bag_of_words, 3) == 'little'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "How many words appear n times in the text?\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Input is taken as a `dict` and an `int` n, where n is the number of times the word appears in the text\n",
    "* Count the number of words that appear n times in the text\n",
    "* Output should be the count as an `int`\n",
    "\n",
    "Example: \n",
    "```python\n",
    "word_frequency_count(bag = {'apple': 30, 'orange': 12, 'pear': 50, 'banana': 12}, 12)\n",
    "\n",
    ">>> 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "def word_frequency_count(bag, n):\n",
    "    # your code here\n",
    "    common_words = []\n",
    "    for k,v in bag.items():\n",
    "        if v == n:\n",
    "            common_words.append(k)\n",
    "    freq = len(common_words)\n",
    "    return freq\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_count(bag_of_words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_count(bag_of_words, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected output:**_\n",
    "\n",
    "```python\n",
    "most_common_word(bag_of_words, 5) == 97\n",
    "most_common_word(bag_of_words, 8) == 49\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
