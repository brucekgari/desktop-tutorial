{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb348ef8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mastering Multiple Models\n",
    "\n",
    "**In this webinar:**\n",
    "\n",
    "1. Tree Based Classifiers\n",
    "    - Decision Trees\n",
    "    - Random Forest\n",
    "2. Support Vector Classifier\n",
    "3. Naive Bayes Classifier\n",
    "4. K-Nearest Neighbors Classifier\n",
    "5. K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeabbbc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Objectives:**\n",
    "\n",
    "- Understand and explain (at a high level) how each classifier works\n",
    "- Apply code for more efficient model training and testing\n",
    "- Apply K-fold cross validation to any trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7747711e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a749ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Tree Based Classifiers\n",
    "\n",
    "### 1.1 Decision Tree Classifier\n",
    "\n",
    "Decision tree classifiers work in very much the same way as they do for regression. The tree is constructed from a root node, and by continous binary splitting, creating decision nodes and leaf nodes. New data points pass through the nodes in the tree to reach a prediction for that data point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7016a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://images.datacamp.com/image/upload/f_auto,q_auto:best/v1545933328/output_65_0_jteora.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531facc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can alter some properties of the tree to see how it affects performance - such as `max_depth` (the maximum depth of the tree). You can read about all the different parameters of Decision Trees [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "Overfitting is a common problem for decision trees, as it can become too fitted and detailed to the training data, therefore dropping in performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b11cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2 Random Forest Classifier\n",
    "\n",
    "The random forest is an ensemble method built on decision trees. It is a non-parametric algorithm that aggregates the result of multiple decision trees. This help to alleviate the issue of overfitting experienced by using a decision tree alone. The output is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d35f60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:592/1*i0o8mjFfCn-uD79-F1Cqkw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978f943",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the Random Forest algorithm we can also tune parameters to improve the model, such as `n_estimators` (number of trees to include in forest) or `max_depth` (maximum depth of the tree, similar to what we use with a Decision Tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc7ee8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Support Vector Classifier\n",
    "\n",
    "SVM plots data as points in space and will look for the widest, clearest gap between points belonging to each class in your data. The boundary created by the SVM here is known as the **hyperplane**. When a new data point is encountered, it will use this hyperplane and the margin around it to classify the new data point, depending on which side of the margin it is closest to.\n",
    "\n",
    "The term 'support vectors' are the data points that line the edge of the margin of the hyperplane.\n",
    "\n",
    "SVM is useful when working with data that is not linearly separable or has noise. It can handle both categorical and continuous data, and can handle both linear and non-linear problems. SVM is less prone to overfitting and can handle high dimensional data (data with many features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84229cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed954fbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes makes use of Bayes' Theorem to classify data points into classes, based on probabilities. \n",
    "\n",
    "Naive Bayes assumes that all features are independent of each other, an assumption that often does not hold in the real world (hence the name 'Naive'!). Despite this, it's still a popular algorithm, and is widely used. It can be used for both categorical and continuous data, and can work well with a smaller amount of training data if the assumption of independence is upheld."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc6d15",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://i0.wp.com/innovationyourself.com/wp-content/uploads/2023/10/Implementing-Naive-Bayes-Classification-using-Python-1-1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87c1ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$P(spam \\mid words) = \\frac{ P(words \\mid spam) * P(spam) }{ P(words) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c598095",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. K-Nearest Neighbors Classifier\n",
    "\n",
    "KNN find the K nearest data points to the given input and predicts the target variable based on the mode or mean of the output values of those neighbours.\n",
    "\n",
    "KNN is useful when working with data that is not linearly separable and when the model complexity is not a concern. It can handle both categorical and continuous data, and provides a simple method of imputation for missing values. KNN can however be computationally expensive and lacks robustness to different distance metrics (such as scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce92e9e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Choose K (number of neighbours)\n",
    "- Choose distance metric, e.g. Euclidean distance\n",
    "- For each data point in the testing data, calculate the distance (using your chosen distance metric) between the test point and training observations\n",
    "- Find labels of the K closest data points to the testing point\n",
    "- Assign most frequent (mode) class label to the testing point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322df0f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:505/0*2_qzcm2gSe9l67aI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001cc680",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Building multiple models & k-fold cross validation!\n",
    "\n",
    "We already are quite familiar with building models and the steps we need to go through to create these models, train them and make predictions. So let's try this out in a more efficient way, where we work on multiple models at once - saving us time and saving us from writing very similar code over and over again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "021eb6cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>kids like story BUT while i really wanted a bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Bought this used and it came in great conditio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Every story and book about Corduroy is Fantast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>I purchased this book for my first grade class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Having spent numerous years in an elementary s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText\n",
       "0        4  kids like story BUT while i really wanted a bo...\n",
       "1        4  Bought this used and it came in great conditio...\n",
       "2        5  Every story and book about Corduroy is Fantast...\n",
       "3        5  I purchased this book for my first grade class...\n",
       "4        5  Having spent numerous years in an elementary s..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews = pd.read_csv('amazon_reviews.csv')\n",
    "df_reviews = df_reviews.drop(columns = 'Unnamed: 0').dropna()\n",
    "\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdec65e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at the distribution of ratings in our data\n",
    "\n",
    "ratings = df_reviews[['overall', 'reviewText']].groupby('overall').count()\n",
    "\n",
    "ratings.sort_values('overall', ascending = False).plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e542af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out some more EDA to explore the data! Think: common words? Wordclouds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice balancing the data, see if you can upsample/downsample across all classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defcf360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out some more NLP steps to see if it improves any of them models' performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be88fa57",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a CountVectorizer instance \n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "\n",
    "# Fit and transform the documents, set up X and y\n",
    "X = vectorizer.fit_transform(df_reviews['reviewText'])\n",
    "y = np.array(df_reviews['overall'])\n",
    "\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4ba5f9b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "names = ['Logistic Regression', 'Nearest Neighbors',\n",
    "         'RBF SVM', 'Decision Tree', 'Random Forest',  'Naive Bayes']\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(multi_class = 'ovr'),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10),\n",
    "    MultinomialNB()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c97dbc5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression model...\n",
      "... predicting\n",
      "... scoring\n",
      "Fitting Nearest Neighbors model...\n",
      "... predicting\n",
      "... scoring\n",
      "Fitting RBF SVM model...\n",
      "... predicting\n",
      "... scoring\n",
      "Fitting Decision Tree model...\n",
      "... predicting\n",
      "... scoring\n",
      "Fitting Random Forest model...\n",
      "... predicting\n",
      "... scoring\n",
      "Fitting Naive Bayes model...\n",
      "... predicting\n",
      "... scoring\n",
      "... All done!\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "models = {}\n",
    "confusion = {}\n",
    "class_report = {}\n",
    "\n",
    "\n",
    "for name, clf in zip(names, classifiers): \n",
    "    print ('Fitting {:s} model...'.format(name))\n",
    "    run_time = %timeit -q -o clf.fit(X_train, y_train)\n",
    "\n",
    "    print ('... predicting')\n",
    "    y_pred = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "\n",
    "    print ('... scoring')\n",
    "    accuracy  = metrics.accuracy_score(y_train, y_pred)\n",
    "    precision = metrics.precision_score(y_train, y_pred, average='weighted')\n",
    "    recall    = metrics.recall_score(y_train, y_pred, average='weighted')\n",
    "    f1        = metrics.f1_score(y_train, y_pred, average='weighted')\n",
    "    f1_test   = metrics.f1_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "    # Save the results to dictionaries\n",
    "    models[name] = clf\n",
    "    confusion[name] = metrics.confusion_matrix(y_train, y_pred)\n",
    "    class_report[name] = metrics.classification_report(y_train, y_pred)\n",
    "\n",
    "    results.append([name, accuracy, precision, recall, f1, f1_test, run_time.best])\n",
    "\n",
    "\n",
    "results = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', \n",
    "                                         'F1 Train', 'F1 Test', 'Train Time'])\n",
    "results.set_index('Classifier', inplace= True)\n",
    "\n",
    "print ('... All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cec73f32",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Train</th>\n",
       "      <th>F1 Test</th>\n",
       "      <th>Train Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.901088</td>\n",
       "      <td>0.908648</td>\n",
       "      <td>0.901088</td>\n",
       "      <td>0.892292</td>\n",
       "      <td>0.707333</td>\n",
       "      <td>1.025986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.807803</td>\n",
       "      <td>0.822682</td>\n",
       "      <td>0.807803</td>\n",
       "      <td>0.764061</td>\n",
       "      <td>0.673536</td>\n",
       "      <td>0.001796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBF SVM</th>\n",
       "      <td>0.985119</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.985119</td>\n",
       "      <td>0.984926</td>\n",
       "      <td>0.658826</td>\n",
       "      <td>13.622875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.803926</td>\n",
       "      <td>0.792985</td>\n",
       "      <td>0.803926</td>\n",
       "      <td>0.772123</td>\n",
       "      <td>0.656383</td>\n",
       "      <td>0.000320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.751907</td>\n",
       "      <td>0.718104</td>\n",
       "      <td>0.751907</td>\n",
       "      <td>0.662814</td>\n",
       "      <td>0.652517</td>\n",
       "      <td>0.075273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.742278</td>\n",
       "      <td>0.550977</td>\n",
       "      <td>0.742278</td>\n",
       "      <td>0.632479</td>\n",
       "      <td>0.637475</td>\n",
       "      <td>0.014062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision    Recall  F1 Train   F1 Test  \\\n",
       "Classifier                                                               \n",
       "Logistic Regression  0.901088   0.908648  0.901088  0.892292  0.707333   \n",
       "Naive Bayes          0.807803   0.822682  0.807803  0.764061  0.673536   \n",
       "RBF SVM              0.985119   0.985337  0.985119  0.984926  0.658826   \n",
       "Nearest Neighbors    0.803926   0.792985  0.803926  0.772123  0.656383   \n",
       "Decision Tree        0.751907   0.718104  0.751907  0.662814  0.652517   \n",
       "Random Forest        0.742278   0.550977  0.742278  0.632479  0.637475   \n",
       "\n",
       "                     Train Time  \n",
       "Classifier                       \n",
       "Logistic Regression    1.025986  \n",
       "Naive Bayes            0.001796  \n",
       "RBF SVM               13.622875  \n",
       "Nearest Neighbors      0.000320  \n",
       "Decision Tree          0.075273  \n",
       "Random Forest          0.014062  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('F1 Test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd0e63",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### K-fold Validation\n",
    "\n",
    "A common approach to validation called k-fold cross validation. In its simplest form, k-fold fits a single model to a number of different versions of the test-train split and compares how volatile the results of the model are. In the below image, we use a 5 'folds' for the validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59643e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84afdc3a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63023124 0.66552068 0.67453091 0.66979999 0.62362187 0.65683526\n",
      " 0.65545829 0.66993233 0.67413399 0.67474388]\n"
     ]
    }
   ],
   "source": [
    "model = models['Naive Bayes']\n",
    "print(cross_val_score(model, X, y, scoring = 'f1_weighted', cv = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f40bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Coming up next time:**\n",
    "\n",
    "Hyperparameter tuning & GridSearchCV!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
