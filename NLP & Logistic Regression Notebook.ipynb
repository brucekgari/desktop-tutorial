{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477dabfa-3e4d-4a8a-8d50-1b309466b697",
   "metadata": {
    "id": "477dabfa-3e4d-4a8a-8d50-1b309466b697"
   },
   "source": [
    "# Natural Language Processing + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b91ab-3ef3-4ae1-84c7-3f2496a6f12d",
   "metadata": {
    "id": "0d7b91ab-3ef3-4ae1-84c7-3f2496a6f12d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6934a70-da0d-46b0-8b21-ce88beb3ef12",
   "metadata": {
    "id": "a6934a70-da0d-46b0-8b21-ce88beb3ef12"
   },
   "source": [
    "## What to expect:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81876407-fcb3-4773-b257-9b34bc1e6f8a",
   "metadata": {
    "id": "81876407-fcb3-4773-b257-9b34bc1e6f8a"
   },
   "source": [
    "1. Natural Language Processing\n",
    "2. Introduction to NLTK\n",
    "3. Dataset: Spam Emails\n",
    "4. Text cleaning\n",
    "5. Text Feature Extraction\n",
    "6. Example of using a Sklearn Vectorizer\n",
    "7. Implementing a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75408ab8-49a7-4257-9812-6c5e32af5034",
   "metadata": {
    "id": "75408ab8-49a7-4257-9812-6c5e32af5034"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d119f8ad-c5c1-494e-8a8d-9ed9d3fa331d",
   "metadata": {
    "id": "d119f8ad-c5c1-494e-8a8d-9ed9d3fa331d"
   },
   "outputs": [],
   "source": [
    "# Import the usual...\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65646d0-725c-4638-9f59-d58fab993a64",
   "metadata": {
    "id": "e65646d0-725c-4638-9f59-d58fab993a64"
   },
   "source": [
    "## 1. Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4307018-e20e-4fd6-b434-c9db837c054b",
   "metadata": {
    "id": "e4307018-e20e-4fd6-b434-c9db837c054b"
   },
   "source": [
    "![picture](https://miro.medium.com/v2/resize:fit:1358/0*ZAqYrctJClczGHps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdeaac-8cca-44e8-9678-0ee51ec053c5",
   "metadata": {
    "id": "e5cdeaac-8cca-44e8-9678-0ee51ec053c5"
   },
   "source": [
    "## 3. Dataset: Spam Bam Thank You Ma'am!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9bd853-f520-488d-b508-176bbdc82741",
   "metadata": {
    "id": "be9bd853-f520-488d-b508-176bbdc82741"
   },
   "source": [
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,572 messages, tagged according to being ham (legitimate) or spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a0ee7-5103-4dc7-8563-f9e42a605ec1",
   "metadata": {
    "id": "506a0ee7-5103-4dc7-8563-f9e42a605ec1"
   },
   "source": [
    "<img src='https://64.media.tumblr.com/d49d6e69d3c5a8d086959acf3a2dba2d/tumblr_npzsif8c7H1tjmfrio1_1280.jpg' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd60f5a-7309-4dcc-9786-19989f9bf469",
   "metadata": {
    "id": "ffd60f5a-7309-4dcc-9786-19989f9bf469"
   },
   "outputs": [],
   "source": [
    "# Upload the spam dataset\n",
    "spam = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "# Rename the columns in the dataset\n",
    "spam.rename(columns={'v1': 'Email Type', 'v2': 'Email'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce2ca54-c6bd-4b82-b264-a7c4d6312cf3",
   "metadata": {
    "id": "bce2ca54-c6bd-4b82-b264-a7c4d6312cf3",
    "outputId": "9af69ffd-d812-4f14-eab7-3b7fe7dee1c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's takea look at our feature and element count\n",
    "spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20622dbd-eab6-42d5-bb00-2c14ec88bcb1",
   "metadata": {
    "id": "20622dbd-eab6-42d5-bb00-2c14ec88bcb1",
    "outputId": "28d6e067-3963-431b-99ed-e039bf76dd31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Email</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email Type                                              Email Unnamed: 2  \\\n",
       "0        ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1        ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2       spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3        ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4        ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c43fa3a-37ff-4f51-a632-4243e54f09bd",
   "metadata": {
    "id": "6c43fa3a-37ff-4f51-a632-4243e54f09bd",
    "outputId": "feb9a803-4580-4164-97d4-7b38f5edffc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Email Type       0\n",
       "Email            0\n",
       "Unnamed: 2    5522\n",
       "Unnamed: 3    5560\n",
       "Unnamed: 4    5566\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us take a look at null values and empty columns\n",
    "spam.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "526da688-ad3b-4e30-9df8-7fb42298f44a",
   "metadata": {
    "id": "526da688-ad3b-4e30-9df8-7fb42298f44a"
   },
   "outputs": [],
   "source": [
    "# Some feature engineering\n",
    "spam = spam.drop(columns =['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17bac3da-6232-4899-9ef1-54874123e44e",
   "metadata": {
    "id": "17bac3da-6232-4899-9ef1-54874123e44e",
    "outputId": "e68a892e-f263-4a50-e635-528ee73ea059"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email Type                                              Email\n",
       "0        ham  Go until jurong point, crazy.. Available only ...\n",
       "1        ham                      Ok lar... Joking wif u oni...\n",
       "2       spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3        ham  U dun say so early hor... U c already then say...\n",
       "4        ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That looks much better\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96bca0-89be-4860-bf37-0e9053b8e7a4",
   "metadata": {
    "id": "5b96bca0-89be-4860-bf37-0e9053b8e7a4"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba76664-f751-4cd2-afdb-db4cd74814db",
   "metadata": {
    "id": "dba76664-f751-4cd2-afdb-db4cd74814db"
   },
   "source": [
    "## 2. Inroduction to NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f0ede-8f2c-4495-bd61-6cdbba64aa21",
   "metadata": {
    "id": "4d0f0ede-8f2c-4495-bd61-6cdbba64aa21"
   },
   "source": [
    "The **Natural Language Toolkit (NLTK)** is a powerful and versatile library for natural language processing (NLP) in Python.\n",
    "\n",
    "It provides tools and resources for tasks such as:\n",
    "\n",
    "- **Text Processing**\n",
    "- **Linguistic Analysis**\n",
    "- **Machine learning**\n",
    "\n",
    "With a comprehensive collection of datasets and algorithms, NLTK facilitates tasks like:\n",
    "- Tokenization\n",
    "- Part-of-speech tagging,\n",
    "- Sentiment analysis and more.\n",
    "\n",
    "Its user-friendly interface and extensive documentation make it accessible for beginners, while its scalability and functionality appeal to researchers and professionals. NLTK plays a pivotal role in advancing NLP research and applications, serving as a foundational tool in academia, industry, and the development of cutting-edge language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a52b71-f502-4986-8a46-25bb3e44cade",
   "metadata": {
    "id": "a7a52b71-f502-4986-8a46-25bb3e44cade"
   },
   "source": [
    "```python\n",
    "\n",
    "# Make sure that you have installed the library into your local machine\n",
    "!pip install nltk\n",
    "\n",
    "#import the pakage\n",
    "import nltk\n",
    "\n",
    "# This is an alternative to the using the NLTK downloader\n",
    "nltk.download() # may be expensive for certain laptops.\n",
    "#download directly\n",
    "nltk.download(['punkt','stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e0e2c-0d2e-4661-a0e0-7f25474c1a27",
   "metadata": {
    "id": "dc4e0e2c-0d2e-4661-a0e0-7f25474c1a27"
   },
   "source": [
    "###  Natural Language Toolkit (NLTK) downloader tool:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ca616-1bbb-4653-bf7a-57919a819ca6",
   "metadata": {
    "id": "e35ca616-1bbb-4653-bf7a-57919a819ca6"
   },
   "source": [
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/nltk_downloader.png?raw=true\" width=50%/>\n",
    "\n",
    "Use it to navigate to the item we need to download:\n",
    "- stopwords corpus (Corpora tab)\n",
    "- punkt tokenizer models (Models tab)\n",
    "\n",
    "Navigate to these, click the download button, and exit the downloader when finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a77602-4e4e-475e-b6cb-c13b966bcbb9",
   "metadata": {
    "id": "87a77602-4e4e-475e-b6cb-c13b966bcbb9"
   },
   "source": [
    "## 3. Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464c525-dc2d-40ea-a9f7-86b156840601",
   "metadata": {
    "id": "3464c525-dc2d-40ea-a9f7-86b156840601"
   },
   "source": [
    "Text cleaning is a crucial preprocessing step in Natural Language Processing (NLP) that involves transforming raw text data into a format suitable for analysis. Here are key steps in text cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0d314-2c30-4472-b74d-b20e19f1d43f",
   "metadata": {
    "id": "3be0d314-2c30-4472-b74d-b20e19f1d43f"
   },
   "source": [
    "![gif](https://media2.giphy.com/media/FHEjBpiqMwSuA/200.webp?cid=ecf05e47g0sxxtxnpwujotm7zj2dqex8is0jyahlqzwh6kvl&ep=v1_gifs_search&rid=200.webp&ct=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372a8e9-91db-4e8a-ab84-946a399623d9",
   "metadata": {
    "id": "e372a8e9-91db-4e8a-ab84-946a399623d9"
   },
   "source": [
    "### Removing Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126ba99-9524-40f6-aa95-b2984ff26d23",
   "metadata": {
    "id": "1126ba99-9524-40f6-aa95-b2984ff26d23"
   },
   "source": [
    "Removing noise, such as **special characters**, is a crucial step in text preprocessing for Natural Language Processing (NLP). Special characters, symbols, and punctuation may introduce **unnecessary complexity** and hinder analysis. By systematically **eliminating these elements**, the text becomes cleaner and **more focused on the essential information**. Striking a balance between maintaining meaningful content and eliminating distracting noise is essential for extracting valuable insights from textual data in diverse domains, from social media analytics to scientific literature mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae14b0-b087-4c1d-9343-5b2c925b47dc",
   "metadata": {
    "id": "30ae14b0-b087-4c1d-9343-5b2c925b47dc"
   },
   "source": [
    "### <font color='maroon'>Regular Expression</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97595d-f5be-429c-83c6-768bde102403",
   "metadata": {
    "id": "ad97595d-f5be-429c-83c6-768bde102403"
   },
   "source": [
    "Regular expressions (regex or regexp) are powerful tools for pattern matching and string manipulation. The re module provides functions and methods to work with regular expressions in Python. There are a quite a few useful tools from the library and I encourage you to check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64911d-6288-4d7a-beac-ce8ba55ca34e",
   "metadata": {
    "id": "0d64911d-6288-4d7a-beac-ce8ba55ca34e"
   },
   "source": [
    "### Removing Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c32238-f3be-4b1f-974b-73c4f00e1d19",
   "metadata": {
    "id": "e4c32238-f3be-4b1f-974b-73c4f00e1d19"
   },
   "source": [
    "```python\n",
    "# Importing the Regular Expressions Library\n",
    "import re\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6a1f3b-ab44-4394-a33e-dcfacd8f0931",
   "metadata": {
    "id": "ea6a1f3b-ab44-4394-a33e-dcfacd8f0931"
   },
   "outputs": [],
   "source": [
    "# Example text containing email addresses\n",
    "text_with_emails = \"\"\"\n",
    "    John Doe's email is john.doe@example.com,\n",
    "    and Jane Smith can be reached at jane.smith@gmail.com.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2e2e7a-a6ed-4651-9edb-87d5bfda414e",
   "metadata": {
    "id": "6e2e2e7a-a6ed-4651-9edb-87d5bfda414e"
   },
   "outputs": [],
   "source": [
    "# Define a regex pattern for matching email addresses\n",
    "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a8f092-ec58-40c3-be60-34783972c84c",
   "metadata": {
    "id": "c9a8f092-ec58-40c3-be60-34783972c84c"
   },
   "outputs": [],
   "source": [
    "# Use re.sub() to replace email addresses with an empty string\n",
    "text_without_emails = re.sub(email_pattern, '', text_with_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502c340d-32b5-4e0f-8c0b-6d7f9e0746fb",
   "metadata": {
    "id": "502c340d-32b5-4e0f-8c0b-6d7f9e0746fb",
    "outputId": "83caac00-c203-4aa8-f659-a2dcc094b7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    John Doe's email is ,\n",
      "    and Jane Smith can be reached at .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the text without emails\n",
    "print(text_without_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4544e9-f834-467b-b446-7e3d8a5fe478",
   "metadata": {
    "id": "7f4544e9-f834-467b-b446-7e3d8a5fe478"
   },
   "source": [
    "### Removing  Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2fb1630-e2e4-4e6d-8acc-6d44c1aea05f",
   "metadata": {
    "id": "e2fb1630-e2e4-4e6d-8acc-6d44c1aea05f"
   },
   "outputs": [],
   "source": [
    "# Create a new column\n",
    "spam['Email_Lower'] = spam['Email'].copy().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5840bc9-54c2-4c74-84ad-77276b0ead1c",
   "metadata": {
    "id": "f5840bc9-54c2-4c74-84ad-77276b0ead1c",
    "outputId": "1b2dd037-9b64-423f-b00c-baa8650f39fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_Lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email Type                                              Email  \\\n",
       "0        ham  Go until jurong point, crazy.. Available only ...   \n",
       "1        ham                      Ok lar... Joking wif u oni...   \n",
       "2       spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        ham  U dun say so early hor... U c already then say...   \n",
       "4        ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         Email_Lower  \n",
       "0  go until jurong point, crazy.. available only ...  \n",
       "1                      ok lar... joking wif u oni...  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3  u dun say so early hor... u c already then say...  \n",
       "4  nah i don't think he goes to usf, he lives aro...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9d11298-941d-4a93-b342-6b305206d153",
   "metadata": {
    "id": "b9d11298-941d-4a93-b342-6b305206d153",
    "outputId": "6295526f-6473-41b2-a593-84e84078807f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005. text fa to 87121 to receive entry question(std txt rate)t&c's apply 08452810075over18's\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam['Email_Lower'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "effdffa4-bbbd-45c7-9fbf-b13050d1490d",
   "metadata": {
    "id": "effdffa4-bbbd-45c7-9fbf-b13050d1490d",
    "outputId": "96d49612-eb7f-4084-c202-5921dd30c49d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de705de2-15fb-4fa1-8caf-7f946c8b20a1",
   "metadata": {
    "id": "de705de2-15fb-4fa1-8caf-7f946c8b20a1"
   },
   "outputs": [],
   "source": [
    "def punctuation_remover(email):\n",
    "    return ''.join([l for l in email if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d24360d9-bfb5-49b5-bc29-51b8e4ee2350",
   "metadata": {
    "id": "d24360d9-bfb5-49b5-bc29-51b8e4ee2350",
    "outputId": "db600c89-d7b8-4865-bde7-1167e9b46a2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam['Email_no_punc'] = spam['Email_Lower'].apply(punctuation_remover)\n",
    "spam['Email_no_punc'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd9f3651-42b7-4965-b75f-8e7159bd30a5",
   "metadata": {
    "id": "dd9f3651-42b7-4965-b75f-8e7159bd30a5",
    "outputId": "27c969ba-396d-49fa-8428-c0908145b3b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_Lower</th>\n",
       "      <th>Email_no_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email Type                                              Email  \\\n",
       "0        ham  Go until jurong point, crazy.. Available only ...   \n",
       "1        ham                      Ok lar... Joking wif u oni...   \n",
       "2       spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        ham  U dun say so early hor... U c already then say...   \n",
       "4        ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         Email_Lower  \\\n",
       "0  go until jurong point, crazy.. available only ...   \n",
       "1                      ok lar... joking wif u oni...   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3  u dun say so early hor... u c already then say...   \n",
       "4  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Email_no_punc  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31b7dd-4cd6-4585-9a9e-eb722c1e644f",
   "metadata": {
    "id": "ba31b7dd-4cd6-4585-9a9e-eb722c1e644f"
   },
   "source": [
    "### Tokanization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d637b624-a694-4255-9a1d-5f840f286660",
   "metadata": {
    "id": "d637b624-a694-4255-9a1d-5f840f286660"
   },
   "source": [
    "Tokenization, in the realm of Natural Language Processing (NLP) and machine learning, refers to the process of **converting a sequence of text into smaller parts**, known as **tokens**. These tokens can be as small as characters or as long as words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9ba62-a183-4e69-8e0f-db05e863c793",
   "metadata": {
    "id": "a7e9ba62-a183-4e69-8e0f-db05e863c793"
   },
   "source": [
    "![gif](https://media0.giphy.com/media/VGVwLultLZjrrssAak/200w.webp?cid=ecf05e47i3nnw7s6mxydile8vewidzswfodj9j6yoji9msjk&ep=v1_gifs_search&rid=200w.webp&ct=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3999dbd-991b-4caa-ab38-216bcdfee081",
   "metadata": {
    "id": "f3999dbd-991b-4caa-ab38-216bcdfee081",
    "outputId": "911adf6f-131e-4b58-e946-ed18bd44655a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'an', 'important', 'step', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "# Download library tools for tokanization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "text = \"Tokenization is an important step in NLP.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0335105-88af-49ee-82f2-97d477e27e7e",
   "metadata": {
    "id": "e0335105-88af-49ee-82f2-97d477e27e7e"
   },
   "source": [
    "#### Let's see how we can tokanize the entire email corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e3b4315-9fb2-4b2a-8dad-bcb711e687c8",
   "metadata": {
    "id": "7e3b4315-9fb2-4b2a-8dad-bcb711e687c8"
   },
   "outputs": [],
   "source": [
    "# Lets work with a more efficient tokanizer\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "spam['Email_tokanized'] = spam['Email_no_punc'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "301e44e6-989a-4593-9461-8099debd593f",
   "metadata": {
    "id": "301e44e6-989a-4593-9461-8099debd593f",
    "outputId": "c6482794-650d-4c97-d37f-9d5a2208df29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_Lower</th>\n",
       "      <th>Email_no_punc</th>\n",
       "      <th>Email_tokanized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email Type                                              Email  \\\n",
       "0        ham  Go until jurong point, crazy.. Available only ...   \n",
       "1        ham                      Ok lar... Joking wif u oni...   \n",
       "2       spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        ham  U dun say so early hor... U c already then say...   \n",
       "4        ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         Email_Lower  \\\n",
       "0  go until jurong point, crazy.. available only ...   \n",
       "1                      ok lar... joking wif u oni...   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3  u dun say so early hor... u c already then say...   \n",
       "4  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Email_no_punc  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                     Email_tokanized  \n",
       "0  [go, until, jurong, point, crazy, available, o...  \n",
       "1                     [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...  \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "703f4a63-0374-4077-b990-be19b0936b13",
   "metadata": {
    "id": "703f4a63-0374-4077-b990-be19b0936b13",
    "outputId": "aa0468f4-6317-4a09-ccac-dd3316df4c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply', '08452810075over18s']\n"
     ]
    }
   ],
   "source": [
    "tokanized = spam['Email_tokanized'].iloc[2]\n",
    "print(tokanized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed41c86-090b-4371-9a5d-e21425bf8bd5",
   "metadata": {
    "id": "2ed41c86-090b-4371-9a5d-e21425bf8bd5"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bacc9-ffca-4e66-aeed-3df324e57ca2",
   "metadata": {
    "id": "9f2bacc9-ffca-4e66-aeed-3df324e57ca2"
   },
   "source": [
    "Stemming in Natural Language Processing (NLP) is a linguistic normalization technique that involves **reducing words** to their base or **root** form. It simplifies word variations, allowing algorithms to treat related words as equivalent. Stemming **enhances text analysis by standardizing vocabulary**, aiding in tasks like text classification and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1850c7f6-b562-4129-ad2e-1b91d6b78a70",
   "metadata": {
    "id": "1850c7f6-b562-4129-ad2e-1b91d6b78a70"
   },
   "source": [
    "![gif](https://i.imgur.com/BAfaGBL.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf19e444-59c5-43d7-9c37-8d80840b184b",
   "metadata": {
    "id": "bf19e444-59c5-43d7-9c37-8d80840b184b"
   },
   "outputs": [],
   "source": [
    "# Stemming packages from nltk\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d91b4de-12e5-48e8-9960-a16dc4057700",
   "metadata": {
    "id": "6d91b4de-12e5-48e8-9960-a16dc4057700"
   },
   "outputs": [],
   "source": [
    "# string example 1\n",
    "red = \"Redness Red Redden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4792d03f-c392-4cce-8326-895271f39d80",
   "metadata": {
    "id": "4792d03f-c392-4cce-8326-895271f39d80"
   },
   "outputs": [],
   "source": [
    "# string example 2\n",
    "snore = \"sleep sleeping sleepiest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afea016b-edec-416e-bb5d-d7a56aa7d6e6",
   "metadata": {
    "id": "afea016b-edec-416e-bb5d-d7a56aa7d6e6",
    "outputId": "d5aed6ba-c916-4b3e-c553-bd5a018960bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep\n",
      "sleep\n",
      "sleepiest\n"
     ]
    }
   ],
   "source": [
    "# find the stem of each word in words, noyice the word sleepiest!\n",
    "stemmer = SnowballStemmer('english')\n",
    "for word in snore.split(): # This will create a list of the strings...\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "256c4c5f-f07d-48f2-9531-569f16e7d4eb",
   "metadata": {
    "id": "256c4c5f-f07d-48f2-9531-569f16e7d4eb",
    "outputId": "1792e8ee-2c0e-4ebf-85f2-82f383598c38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nah I don't think he goes to usf, he lives around here though\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam['Email'].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "516a22eb-9533-4c1e-a9f1-d4fc2fab16aa",
   "metadata": {
    "id": "516a22eb-9533-4c1e-a9f1-d4fc2fab16aa"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1342369942.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/kd/y6587_tj027169vmxzn900sc0000gn/T/ipykernel_2044/1342369942.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print(stemmer.stem(word))''''\u001b[0m\n\u001b[0m                                 \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# split data and stem individual words\n",
    "''''for word in spam['Email_tokanized']:\n",
    "    print(stemmer.stem(word))''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17dc26eb-f4e3-4b1c-8127-a50135f6f75b",
   "metadata": {
    "id": "17dc26eb-f4e3-4b1c-8127-a50135f6f75b"
   },
   "outputs": [],
   "source": [
    "spam['Email_stemmed'] = spam['Email_tokanized'].apply(lambda row: [stemmer.stem(word) for word in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7251d354-39c4-48f6-8543-2370f721fa41",
   "metadata": {
    "id": "7251d354-39c4-48f6-8543-2370f721fa41",
    "outputId": "5b88546f-1f7d-481c-a0fc-6437f1567d6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_Lower</th>\n",
       "      <th>Email_no_punc</th>\n",
       "      <th>Email_tokanized</th>\n",
       "      <th>Email_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, until, jurong, point, crazi, avail, onli,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entri, in, 2, a, wkli, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, so, earli, hor, u, c, alreadi, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, i, dont, think, he, goe, to, usf, he, li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email Type                                              Email  \\\n",
       "0        ham  Go until jurong point, crazy.. Available only ...   \n",
       "1        ham                      Ok lar... Joking wif u oni...   \n",
       "2       spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        ham  U dun say so early hor... U c already then say...   \n",
       "4        ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         Email_Lower  \\\n",
       "0  go until jurong point, crazy.. available only ...   \n",
       "1                      ok lar... joking wif u oni...   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3  u dun say so early hor... u c already then say...   \n",
       "4  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Email_no_punc  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                     Email_tokanized  \\\n",
       "0  [go, until, jurong, point, crazy, available, o...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                       Email_stemmed  \n",
       "0  [go, until, jurong, point, crazi, avail, onli,...  \n",
       "1                       [ok, lar, joke, wif, u, oni]  \n",
       "2  [free, entri, in, 2, a, wkli, comp, to, win, f...  \n",
       "3  [u, dun, say, so, earli, hor, u, c, alreadi, t...  \n",
       "4  [nah, i, dont, think, he, goe, to, usf, he, li...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfece01a-d94e-48cc-899b-9f03216700f3",
   "metadata": {
    "id": "dfece01a-d94e-48cc-899b-9f03216700f3"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200c80f-6773-4cae-98a0-8f9fbde38f3a",
   "metadata": {
    "id": "7200c80f-6773-4cae-98a0-8f9fbde38f3a"
   },
   "source": [
    "### Lematization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda293e2-dd96-4980-b348-09376a18cebf",
   "metadata": {
    "id": "eda293e2-dd96-4980-b348-09376a18cebf"
   },
   "source": [
    "Lemmatization in Natural Language Processing (NLP) involves extracting the linguistic **root or lemma** of a word, providing its canonical form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94162084-e39a-43a0-ae7d-4604670792f9",
   "metadata": {
    "id": "94162084-e39a-43a0-ae7d-4604670792f9"
   },
   "source": [
    "![gif](https://media1.giphy.com/media/12XDYvMJNcmLgQ/200.webp?cid=ecf05e478vd7gtq6jep6wrs9dle004apxt49yvcykoinkcvm&ep=v1_gifs_search&rid=200.webp&ct=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ddbd4c4-810d-4945-af16-f36605f3567d",
   "metadata": {
    "id": "8ddbd4c4-810d-4945-af16-f36605f3567d",
    "outputId": "54e60204-7f6f-4537-9769-f7593b52f81f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/claudiaelliotwilson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "larva\n",
      "goose\n",
      "tulip\n",
      "python\n",
      "great\n",
      "great\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize different words\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"larvae\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"tulips\"))\n",
    "print(lemmatizer.lemmatize(\"pythons\"))\n",
    "print(lemmatizer.lemmatize(\"greater\", pos=\"a\"))  # \"a\" indicates adjective\n",
    "print(lemmatizer.lemmatize(\"greatest\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"ran\", pos='v'))  # \"v\" indicates verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de2485a0-b3d8-4448-bafa-8e9a9708f8a4",
   "metadata": {
    "id": "de2485a0-b3d8-4448-bafa-8e9a9708f8a4",
    "outputId": "1f36684f-f2bf-435a-e897-110e0216452c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/claudiaelliotwilson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_Lower</th>\n",
       "      <th>Email_no_punc</th>\n",
       "      <th>Email_tokanized</th>\n",
       "      <th>Email_stemmed</th>\n",
       "      <th>Email_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, until, jurong, point, crazi, avail, onli,...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entri, in, 2, a, wkli, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, so, earli, hor, u, c, alreadi, t...</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, i, dont, think, he, goe, to, usf, he, li...</td>\n",
       "      <td>[nah, i, dont, think, he, go, to, usf, he, lif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email Type                                              Email  \\\n",
       "0        ham  Go until jurong point, crazy.. Available only ...   \n",
       "1        ham                      Ok lar... Joking wif u oni...   \n",
       "2       spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        ham  U dun say so early hor... U c already then say...   \n",
       "4        ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         Email_Lower  \\\n",
       "0  go until jurong point, crazy.. available only ...   \n",
       "1                      ok lar... joking wif u oni...   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3  u dun say so early hor... u c already then say...   \n",
       "4  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Email_no_punc  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                     Email_tokanized  \\\n",
       "0  [go, until, jurong, point, crazy, available, o...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                       Email_stemmed  \\\n",
       "0  [go, until, jurong, point, crazi, avail, onli,...   \n",
       "1                       [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, in, 2, a, wkli, comp, to, win, f...   \n",
       "3  [u, dun, say, so, earli, hor, u, c, alreadi, t...   \n",
       "4  [nah, i, dont, think, he, goe, to, usf, he, li...   \n",
       "\n",
       "                                    Email_lemmatized  \n",
       "0  [go, until, jurong, point, crazy, available, o...  \n",
       "1                     [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...  \n",
       "4  [nah, i, dont, think, he, go, to, usf, he, lif...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download the WordNet dataset\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Create WordNetLemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize each word in the list\n",
    "spam['Email_lemmatized'] = spam['Email_tokanized'].apply(lambda row: [lemmatizer.lemmatize(word) for word in row])\n",
    "\n",
    "# Print the lemmatized words\n",
    "spam.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b61131-6064-4ad1-a909-fa7eca2b67d8",
   "metadata": {
    "id": "33b61131-6064-4ad1-a909-fa7eca2b67d8"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a73ac-9592-492a-a93e-2b82bb402425",
   "metadata": {
    "id": "d65a73ac-9592-492a-a93e-2b82bb402425"
   },
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e92e30-c6f4-4ff0-bd92-d4debc620f09",
   "metadata": {
    "id": "d9e92e30-c6f4-4ff0-bd92-d4debc620f09"
   },
   "source": [
    "![gif](https://media3.giphy.com/media/2WNQ0N41BK5Us/200w.webp?cid=ecf05e47n7g6qrvr4lbv5ni8u2twz2zuxtudsnct5hiwpovl&ep=v1_gifs_search&rid=200w.webp&ct=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf71ca8-4611-484a-9a69-de866837dd11",
   "metadata": {
    "id": "fbf71ca8-4611-484a-9a69-de866837dd11"
   },
   "source": [
    "Stop words in Natural Language Processing (NLP) are common words, such as \"the,\" \"and,\" and \"is,\" that are frequently used but often add little semantic meaning to a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10a60e4c-46f1-4527-a16d-fdfea8051222",
   "metadata": {
    "id": "10a60e4c-46f1-4527-a16d-fdfea8051222"
   },
   "outputs": [],
   "source": [
    "# Import stopwords library of words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b51d448a-5da0-4c78-bd38-1a5cb1b9e450",
   "metadata": {
    "id": "b51d448a-5da0-4c78-bd38-1a5cb1b9e450",
    "outputId": "f55c6261-3408-44ad-9d9c-5d34104f4b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Lets take a look!\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb701a91-6d31-479d-991e-7ff1e88b0592",
   "metadata": {
    "id": "bb701a91-6d31-479d-991e-7ff1e88b0592",
    "outputId": "81748e81-6cfc-49bd-8361-f34933b01fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is an example sentence with some stop words.\n",
      "After Removing Stop Words: example sentence stop words .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/claudiaelliotwilson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK stop words dataset\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence with some stop words.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# Print the result\n",
    "print(\"Original Text:\", text)\n",
    "print(\"After Removing Stop Words:\", ' '.join(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dcec976-f857-446b-9330-8b37a423c998",
   "metadata": {
    "id": "1dcec976-f857-446b-9330-8b37a423c998",
    "outputId": "89994e9e-f008-4e0c-9518-e3d19a59700e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/claudiaelliotwilson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nah', 'i', 'dont', 'think', 'he', 'go', 'to', 'usf', 'he', 'life', 'around', 'here', 'though']\n",
      "['nah', 'dont', 'think', 'go', 'usf', 'life', 'around', 'though']\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK stop words dataset\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "spam['no_stop_words'] = spam['Email_lemmatized'].apply(lambda words: [word for word in words if word not in stopwords.words('english')])\n",
    "\n",
    "# Print the result\n",
    "print(spam['Email_lemmatized'].iloc[4])\n",
    "print(spam['no_stop_words'].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cace7f-efe4-4659-a0de-94ee3f209031",
   "metadata": {
    "id": "37cace7f-efe4-4659-a0de-94ee3f209031"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1c66a-560d-4e40-bf27-38c94afa5f28",
   "metadata": {
    "id": "bdc1c66a-560d-4e40-bf27-38c94afa5f28"
   },
   "source": [
    "## 5. Text Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06179f83-b9e9-4acb-9523-b42265a8508b",
   "metadata": {
    "id": "06179f83-b9e9-4acb-9523-b42265a8508b"
   },
   "source": [
    "Text extraction in Natural Language Processing (NLP) involves **isolating and retrieving relevant information** from *unstructured* text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb325a-e6e8-4ed1-96a6-ed8d24f20d2c",
   "metadata": {
    "id": "9efb325a-e6e8-4ed1-96a6-ed8d24f20d2c"
   },
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf7fc4-ea75-4b34-bce9-6c3bace1cbf1",
   "metadata": {
    "id": "10cf7fc4-ea75-4b34-bce9-6c3bace1cbf1"
   },
   "source": [
    "Focuses on the frequency of words in a document, creating a sparse matrix or a dictionary where each unique word is a feature, and its frequency is the corresponding value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5a18e-e07b-4882-b204-efdd861c4c21",
   "metadata": {
    "id": "f4d5a18e-e07b-4882-b204-efdd861c4c21"
   },
   "source": [
    "![gif](https://media0.giphy.com/media/26tkl6oesuCt5akbC/giphy.webp?cid=ecf05e47e3t1n6o72gxowf3cea64f8vc3venhga5mfxy5lbq&ep=v1_gifs_search&rid=giphy.webp&ct=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15588067-a3c1-4076-b457-1938561064ca",
   "metadata": {
    "id": "15588067-a3c1-4076-b457-1938561064ca",
    "outputId": "a4064c32-fe7e-4fe1-8a38-ead58800fb3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'is': 1, 'a': 2, 'simple': 1, 'example': 1, 'of': 2, 'bag': 1, 'words': 1, 'representation.': 1}\n"
     ]
    }
   ],
   "source": [
    "# Sample document\n",
    "document = \"This is a simple example of a Bag of Words representation.\"\n",
    "\n",
    "# Tokenize the document into words\n",
    "words = document.lower().split()\n",
    "\n",
    "# Create a Bag of Words representation using a dictionary\n",
    "bow_representation = {}\n",
    "for word in words:\n",
    "    bow_representation[word] = bow_representation.get(word, 0) + 1\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(bow_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9945e55-a56b-4da9-972c-87d88ae4ec6f",
   "metadata": {
    "id": "d9945e55-a56b-4da9-972c-87d88ae4ec6f",
    "outputId": "43b156b4-c139-456a-fc95-9ff6955c4a2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['free',\n",
       " 'entry',\n",
       " '2',\n",
       " 'wkly',\n",
       " 'comp',\n",
       " 'win',\n",
       " 'fa',\n",
       " 'cup',\n",
       " 'final',\n",
       " 'tkts',\n",
       " '21st',\n",
       " 'may',\n",
       " '2005',\n",
       " 'text',\n",
       " 'fa',\n",
       " '87121',\n",
       " 'receive',\n",
       " 'entry',\n",
       " 'questionstd',\n",
       " 'txt',\n",
       " 'ratetcs',\n",
       " 'apply',\n",
       " '08452810075over18s']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam['no_stop_words'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bb444bd-ca28-4f6e-8e85-5916ec94014c",
   "metadata": {
    "id": "9bb444bd-ca28-4f6e-8e85-5916ec94014c",
    "outputId": "477a5565-1615-4a3c-f045-d08765780d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'free': 1, 'entry': 2, '2': 1, 'wkly': 1, 'comp': 1, 'win': 1, 'fa': 2, 'cup': 1, 'final': 1, 'tkts': 1, '21st': 1, 'may': 1, '2005': 1, 'text': 1, '87121': 1, 'receive': 1, 'questionstd': 1, 'txt': 1, 'ratetcs': 1, 'apply': 1, '08452810075over18s': 1}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the email into words\n",
    "words = spam['no_stop_words'].iloc[2]\n",
    "\n",
    "# Create a Bag of Words representation using a dictionary\n",
    "bow_representation = {}\n",
    "for word in words:\n",
    "    bow_representation[word] = bow_representation.get(word, 0) + 1\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(bow_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b5510-b56a-42f1-afa1-4c87e8298767",
   "metadata": {
    "id": "130b5510-b56a-42f1-afa1-4c87e8298767"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74384c-a8b8-48eb-8895-5c5e445313e7",
   "metadata": {
    "id": "9d74384c-a8b8-48eb-8895-5c5e445313e7"
   },
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4574ac3-451e-48f1-a5a4-671b63a5ba19",
   "metadata": {
    "id": "a4574ac3-451e-48f1-a5a4-671b63a5ba19"
   },
   "source": [
    "N-grams in Natural Language Processing (NLP) are sequential **word combinations** of 'n' length, where 'n' represents the number of words in each **grouping**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5d9325a-c5c4-4427-b675-6f782a037e56",
   "metadata": {
    "id": "b5d9325a-c5c4-4427-b675-6f782a037e56",
    "outputId": "eb705963-1dc8-4a59-f395-71b12273dd25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nah dont', 'dont think', 'think go', 'go usf', 'usf life', 'life around', 'around though', 'nah dont think', 'dont think go', 'think go usf', 'go usf life', 'usf life around', 'life around though']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def word_grams(words, min_n=1, max_n=4):\n",
    "    list_1 = []\n",
    "    for n in range(min_n, max_n + 1):  # corrected the range\n",
    "        for ngram in ngrams(words, n):\n",
    "            list_1.append(' '.join(str(i) for i in ngram))\n",
    "    return list_1\n",
    "\n",
    "# Example use case from spam dataset:\n",
    "words = spam['no_stop_words'].iloc[4]\n",
    "\n",
    "\n",
    "result = word_grams(words, min_n=2, max_n=3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd7f3e-2f95-44d2-ba67-67e2198af53e",
   "metadata": {
    "id": "cebd7f3e-2f95-44d2-ba67-67e2198af53e"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15704eb0-6d2b-4a2b-9519-d85e991cafc3",
   "metadata": {
    "id": "15704eb0-6d2b-4a2b-9519-d85e991cafc3"
   },
   "source": [
    "## 6. Sklearn Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49d6cb-172c-4281-93ca-98490dabef08",
   "metadata": {
    "id": "7e49d6cb-172c-4281-93ca-98490dabef08"
   },
   "source": [
    "Luckily for us we have tools that make it much easier to process text!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f736d-104a-4d18-882c-58c615357c58",
   "metadata": {
    "id": "666f736d-104a-4d18-882c-58c615357c58"
   },
   "source": [
    "Scikit-learn provides several text vectorizers for converting text data into numerical vectors, commonly used in Natural Language Processing (NLP) tasks. Here are two popular text vectorizers from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f0b4043-46fc-47aa-87ce-b9b1b1b7aaaa",
   "metadata": {
    "id": "7f0b4043-46fc-47aa-87ce-b9b1b1b7aaaa",
    "outputId": "33ad0e54-3355-45ae-816f-3c365959b26d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (unique words in the corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the CountVectorizer matrix and feature names\n",
    "print(\"CountVectorizer Matrix:\")\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcc50354-bda7-4edb-bd25-dc63b77a718f",
   "metadata": {
    "id": "fcc50354-bda7-4edb-bd25-dc63b77a718f",
    "outputId": "6113f85e-c9e7-4abd-cfb8-dd41a4d5028a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this\n",
       "0    0         1      1   1    0       0    1      0     1\n",
       "1    0         2      0   1    0       1    1      0     1\n",
       "2    1         0      0   1    1       0    1      1     1\n",
       "3    0         1      1   1    0       0    1      0     1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets convert to a dataframe!\n",
    "matrix = pd.DataFrame(X.toarray())\n",
    "#Add column names\n",
    "matrix.columns = feature_names\n",
    "# Let's have a look\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c255b2c-7cd5-4eb2-a92f-aa87cdc7bcad",
   "metadata": {
    "id": "0c255b2c-7cd5-4eb2-a92f-aa87cdc7bcad"
   },
   "outputs": [],
   "source": [
    "spam['new_text_cleaned'] = spam['no_stop_words'].apply(lambda words: ' '.join(words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ea3521c-8477-44ca-afea-b9fd42c8c77a",
   "metadata": {
    "id": "4ea3521c-8477-44ca-afea-b9fd42c8c77a",
    "outputId": "5dc8bf72-4d23-46a0-bb15-273522934042"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_Lower</th>\n",
       "      <th>Email_no_punc</th>\n",
       "      <th>Email_tokanized</th>\n",
       "      <th>Email_stemmed</th>\n",
       "      <th>Email_lemmatized</th>\n",
       "      <th>no_stop_words</th>\n",
       "      <th>new_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, until, jurong, point, crazi, avail, onli,...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entri, in, 2, a, wkli, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, so, earli, hor, u, c, alreadi, t...</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, i, dont, think, he, goe, to, usf, he, li...</td>\n",
       "      <td>[nah, i, dont, think, he, go, to, usf, he, lif...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email Type                                              Email  \\\n",
       "0        ham  Go until jurong point, crazy.. Available only ...   \n",
       "1        ham                      Ok lar... Joking wif u oni...   \n",
       "2       spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        ham  U dun say so early hor... U c already then say...   \n",
       "4        ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         Email_Lower  \\\n",
       "0  go until jurong point, crazy.. available only ...   \n",
       "1                      ok lar... joking wif u oni...   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3  u dun say so early hor... u c already then say...   \n",
       "4  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Email_no_punc  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                     Email_tokanized  \\\n",
       "0  [go, until, jurong, point, crazy, available, o...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                       Email_stemmed  \\\n",
       "0  [go, until, jurong, point, crazi, avail, onli,...   \n",
       "1                       [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, in, 2, a, wkli, comp, to, win, f...   \n",
       "3  [u, dun, say, so, earli, hor, u, c, alreadi, t...   \n",
       "4  [nah, i, dont, think, he, goe, to, usf, he, li...   \n",
       "\n",
       "                                    Email_lemmatized  \\\n",
       "0  [go, until, jurong, point, crazy, available, o...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, go, to, usf, he, lif...   \n",
       "\n",
       "                                       no_stop_words  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, early, hor, u, c, already, say]   \n",
       "4  [nah, dont, think, go, usf, life, around, though]   \n",
       "\n",
       "                                    new_text_cleaned  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4           nah dont think go usf life around though  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ab0e244-6dcf-4394-aeca-30e92880c8bf",
   "metadata": {
    "id": "7ab0e244-6dcf-4394-aeca-30e92880c8bf",
    "outputId": "838d4100-71a6-471f-bfa9-a494fe523bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X_spam = vectorizer.fit_transform(spam['new_text_cleaned'])\n",
    "\n",
    "# Get the feature names (unique words in the corpus)\n",
    "feature_names_spam = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the CountVectorizer matrix and feature names\n",
    "print(\"CountVectorizer Matrix:\")\n",
    "print(X_spam.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aef8e259-99a7-4603-9af3-3e89034bb14a",
   "metadata": {
    "id": "aef8e259-99a7-4603-9af3-3e89034bb14a",
    "outputId": "0a27a55e-dd97-48d6-83cc-a97b7b02ba1c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ûthanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 8830 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      008704050406  0089my  0121  01223585236  01223585334  0125698789  02  \\\n",
       "0                0       0     0            0            0           0   0   \n",
       "1                0       0     0            0            0           0   0   \n",
       "2                0       0     0            0            0           0   0   \n",
       "3                0       0     0            0            0           0   0   \n",
       "4                0       0     0            0            0           0   0   \n",
       "...            ...     ...   ...          ...          ...         ...  ..   \n",
       "5567             0       0     0            0            0           0   0   \n",
       "5568             0       0     0            0            0           0   0   \n",
       "5569             0       0     0            0            0           0   0   \n",
       "5570             0       0     0            0            0           0   0   \n",
       "5571             0       0     0            0            0           0   0   \n",
       "\n",
       "      020603  0207  02070836089  ...  ìï  ìïll  ûthanks  ûªm  ûªt  ûªve  ûï  \\\n",
       "0          0     0            0  ...   0     0        0    0    0     0   0   \n",
       "1          0     0            0  ...   0     0        0    0    0     0   0   \n",
       "2          0     0            0  ...   0     0        0    0    0     0   0   \n",
       "3          0     0            0  ...   0     0        0    0    0     0   0   \n",
       "4          0     0            0  ...   0     0        0    0    0     0   0   \n",
       "...      ...   ...          ...  ...  ..   ...      ...  ...  ...   ...  ..   \n",
       "5567       0     0            0  ...   0     0        0    0    0     0   0   \n",
       "5568       0     0            0  ...   0     0        0    0    0     0   0   \n",
       "5569       0     0            0  ...   0     0        0    0    0     0   0   \n",
       "5570       0     0            0  ...   0     0        0    0    0     0   0   \n",
       "5571       0     0            0  ...   0     0        0    0    0     0   0   \n",
       "\n",
       "      ûïharry  ûò  ûówell  \n",
       "0           0   0       0  \n",
       "1           0   0       0  \n",
       "2           0   0       0  \n",
       "3           0   0       0  \n",
       "4           0   0       0  \n",
       "...       ...  ..     ...  \n",
       "5567        0   0       0  \n",
       "5568        0   0       0  \n",
       "5569        0   0       0  \n",
       "5570        0   0       0  \n",
       "5571        0   0       0  \n",
       "\n",
       "[5572 rows x 8830 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets convert to a dataframe!\n",
    "spam_matrix = pd.DataFrame(X_spam.toarray())\n",
    "#Add column names\n",
    "spam_matrix.columns = feature_names_spam\n",
    "# Let's have a look\n",
    "# With all the words available in the dataset can we do more feature engineering?\n",
    "spam_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebb06c-37ed-4414-bc3e-2c205edcfbd1",
   "metadata": {
    "id": "4cebb06c-37ed-4414-bc3e-2c205edcfbd1"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521df46",
   "metadata": {},
   "source": [
    "## 7. Implementing a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b886a1",
   "metadata": {},
   "source": [
    "First things first, we need to encode our y-variable into 1s and 0s for our model to be able to use! **It is important to note that sklearn can do this for you, however, so if you forget to do it, your model will still work!** For the sake of demonstration, we'll use the LabelEncoder to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "spam['Email Type'] = encoder.fit_transform(spam['Email Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0fce6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_Lower</th>\n",
       "      <th>Email_no_punc</th>\n",
       "      <th>Email_tokanized</th>\n",
       "      <th>Email_stemmed</th>\n",
       "      <th>Email_lemmatized</th>\n",
       "      <th>no_stop_words</th>\n",
       "      <th>new_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, until, jurong, point, crazi, avail, onli,...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entri, in, 2, a, wkli, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, so, earli, hor, u, c, alreadi, t...</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, i, dont, think, he, goe, to, usf, he, li...</td>\n",
       "      <td>[nah, i, dont, think, he, go, to, usf, he, lif...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email Type                                              Email  \\\n",
       "0        ham  Go until jurong point, crazy.. Available only ...   \n",
       "1        ham                      Ok lar... Joking wif u oni...   \n",
       "2       spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        ham  U dun say so early hor... U c already then say...   \n",
       "4        ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         Email_Lower  \\\n",
       "0  go until jurong point, crazy.. available only ...   \n",
       "1                      ok lar... joking wif u oni...   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3  u dun say so early hor... u c already then say...   \n",
       "4  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Email_no_punc  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                     Email_tokanized  \\\n",
       "0  [go, until, jurong, point, crazy, available, o...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                       Email_stemmed  \\\n",
       "0  [go, until, jurong, point, crazi, avail, onli,...   \n",
       "1                       [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, in, 2, a, wkli, comp, to, win, f...   \n",
       "3  [u, dun, say, so, earli, hor, u, c, alreadi, t...   \n",
       "4  [nah, i, dont, think, he, goe, to, usf, he, li...   \n",
       "\n",
       "                                    Email_lemmatized  \\\n",
       "0  [go, until, jurong, point, crazy, available, o...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, go, to, usf, he, lif...   \n",
       "\n",
       "                                       no_stop_words  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, early, hor, u, c, already, say]   \n",
       "4  [nah, dont, think, go, usf, life, around, though]   \n",
       "\n",
       "                                    new_text_cleaned  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4           nah dont think go usf life around though  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69331643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_spam.toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec4dfc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'spam', ..., 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(spam['Email Type'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "574eb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dc498c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a Logistic Regression model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "998d4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions!\n",
    "\n",
    "pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df7b4fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52fb024f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0   ham\n",
       "1   ham\n",
       "2   ham\n",
       "3   ham\n",
       "4  spam"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(pred_lr)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e70241b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0   ham\n",
       "1   ham\n",
       "2  spam\n",
       "3   ham\n",
       "4  spam"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b0f55",
   "metadata": {},
   "source": [
    "# Time to check our understanding \n",
    "### Binary Classification + Logistic Regression\n",
    "\n",
    "**1. What method is used to best fit the curve in Logistic Regression?**\n",
    "\n",
    "    a. Least Squares Method\n",
    "    \n",
    "    b. Euclidean Distance\n",
    "    \n",
    "    c. Maximum Likelihood Estimation\n",
    "    \n",
    "    d. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46437cae",
   "metadata": {},
   "source": [
    "**2. T/F: Binary classification refers to situations where the outcome variable has only two potential values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff5900",
   "metadata": {},
   "source": [
    "**3. T/F: When trying to create a classification model, we *don't* need to encode our target variable when it is in a text/object format** (ignoring sklearn's default encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11278075",
   "metadata": {},
   "source": [
    "**4. Complete the following:**\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import ...\n",
    "\n",
    "encoder = ...\n",
    "\n",
    "df['outcome_variable'] = ....fit_transform(...)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648aa7f6",
   "metadata": {},
   "source": [
    "**5. What is the shape of a Logistic Regression curve?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35131266",
   "metadata": {},
   "source": [
    "**6. What is wrong with the following code?**\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0fe8a3",
   "metadata": {},
   "source": [
    "**7. Why can we not fit a straight line (like linear regression) to a classification problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58fc67",
   "metadata": {},
   "source": [
    "**8. Which of the following are true regarding disadvantages of Logistic Regression?**\n",
    "\n",
    "    i. Doesn't handle large number of categorical variables well\n",
    "\n",
    "    ii. Does not work well with non-linearly separable data\n",
    "\n",
    "    iii. Cases of collinearity are worse for Logistic Regression than for Linear Regression\n",
    "\n",
    "\n",
    "- ii only\n",
    "- i & ii\n",
    "- all of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kWenRZTocsna",
   "metadata": {
    "id": "kWenRZTocsna"
   },
   "source": [
    "# NLP Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f361b2-09a0-4277-a248-b9e0e8d9ba3e",
   "metadata": {
    "id": "f3f361b2-09a0-4277-a248-b9e0e8d9ba3e"
   },
   "source": [
    "### Solve the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396b3d4-48c0-4579-a1e4-b64c65b30819",
   "metadata": {
    "id": "c396b3d4-48c0-4579-a1e4-b64c65b30819"
   },
   "source": [
    "#### Question 1: What is the adjective word for the lemma \"cool\" 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d35f7b-6732-4792-bdc6-b1a18d820514",
   "metadata": {
    "id": "12d35f7b-6732-4792-bdc6-b1a18d820514"
   },
   "outputs": [],
   "source": [
    "# Lemmatizer imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the adjective for \"cool\n",
    "print(lemmatizer.lemmatize(\"...\", pos=\"a\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe08c7-703c-4aab-88f0-b5a687198716",
   "metadata": {
    "id": "0cbe08c7-703c-4aab-88f0-b5a687198716"
   },
   "source": [
    "#### Question 2: Tokanize the following sentance 🪙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be292a92-1b5a-468e-8c21-0397859b4fc1",
   "metadata": {
    "id": "be292a92-1b5a-468e-8c21-0397859b4fc1"
   },
   "outputs": [],
   "source": [
    "# Download library tools for tokanization\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"How will you tokanize the following?\"\n",
    "tokens = ...\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b815a2f-7eba-453e-95e4-b9c377196ab4",
   "metadata": {
    "id": "2b815a2f-7eba-453e-95e4-b9c377196ab4"
   },
   "source": [
    "#### Question 3: Create a matrix using the following code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89afc0f-58b4-4403-82ed-70576c0d2d55",
   "metadata": {
    "id": "c89afc0f-58b4-4403-82ed-70576c0d2d55"
   },
   "source": [
    "![gif](https://media2.giphy.com/media/1yvoDVJQsTfHi/200w.webp?cid=ecf05e47rz1ww0wvlzk9x0ak6fniv8curhgxj45gqs9bejl0&ep=v1_gifs_search&rid=200w.webp&ct=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85d811-e020-4003-9bbb-163a26741023",
   "metadata": {
    "id": "ef85d811-e020-4003-9bbb-163a26741023"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a TfidfTransformer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = ...fit_transform(spam['Email'])\n",
    "\n",
    "# Get the feature names (unique words in the corpus)\n",
    "feature_names = ...get_feature_names_out()\n",
    "\n",
    "# Print the CountVectorizer matrix and feature names\n",
    "print(\"CountVectorizer Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806c766-5ab7-410d-83e4-3ebe86eec5a1",
   "metadata": {
    "id": "3806c766-5ab7-410d-83e4-3ebe86eec5a1"
   },
   "source": [
    "## Theory: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517345f-e30a-460f-bfc4-1f455b7ee55a",
   "metadata": {
    "id": "9517345f-e30a-460f-bfc4-1f455b7ee55a"
   },
   "source": [
    "Q1: **What is the difference between stemming and lemmatization?**\n",
    "1. Stemming converts the words to lowercase while lemmatization reduce the word to its root\n",
    "2. Both reduce the word to a root form, but lemmatization uses the context of the word to return a meaningful base form\n",
    "3. Lemmatization splits sentences into individual words, stemming splits words into individual characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c525a-34b9-4894-89c6-8856faefd6a0",
   "metadata": {
    "id": "973c525a-34b9-4894-89c6-8856faefd6a0"
   },
   "source": [
    "Q2: **In the text cleaning process, which comes first: tokenisation or stemming?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1d4af-3606-4f30-bca3-d67159d1bd15",
   "metadata": {
    "id": "18c1d4af-3606-4f30-bca3-d67159d1bd15"
   },
   "source": [
    "Q3: **How many trigrams can we make from this question?**\n",
    "- (hint: ‘tri’ = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6739a-d296-46f4-bd0a-be255b56c09f",
   "metadata": {
    "id": "08d6739a-d296-46f4-bd0a-be255b56c09f"
   },
   "source": [
    "Q4: **True or False: Bag of words only counts one occurrence of each word in the text?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a3fcb-64b9-4c46-88de-d4aac63e6eaf",
   "metadata": {
    "id": "394a3fcb-64b9-4c46-88de-d4aac63e6eaf"
   },
   "source": [
    "Q5: **What package(s) can we import to help us with removing punctuation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27611bba-5391-4ead-82d5-84277a389eb1",
   "metadata": {
    "id": "27611bba-5391-4ead-82d5-84277a389eb1"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472ef50-d3cd-4d01-8e1c-3133af09c6a0",
   "metadata": {
    "id": "4472ef50-d3cd-4d01-8e1c-3133af09c6a0"
   },
   "source": [
    "## Theory: Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1291f-daa9-4286-8614-60f293062776",
   "metadata": {
    "id": "fef1291f-daa9-4286-8614-60f293062776"
   },
   "source": [
    "Q1: **What is the difference between stemming and lemmatization?**\n",
    "1. Stemming converts the words to lowercase while lemmatization reduce the word to its root\n",
    "2. **Both reduce the word to a root form, but lemmatization uses the context of the word to return a meaningful base form**\n",
    "3. Lemmatization splits sentences into individual words, stemming splits words into individual characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1f957-e1db-41ad-b9ef-2e53f841b5b4",
   "metadata": {
    "id": "46e1f957-e1db-41ad-b9ef-2e53f841b5b4"
   },
   "source": [
    "Q2: **In the text cleaning process, which comes first: tokenisation or stemming?**\n",
    "- Tokenisation. Sentences need to be split into their individual words before stemming can work on each of those words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec3caa-881f-4702-ba29-d43bc3d20939",
   "metadata": {
    "id": "97ec3caa-881f-4702-ba29-d43bc3d20939"
   },
   "source": [
    "Q3: **How many trigrams can we make from this question?**\n",
    "- 7\n",
    "  - ‘How many trigrams’\n",
    "  - ‘many trigrams can’\n",
    "  - ‘trigrams can we’\n",
    "  - ‘can we make’\n",
    "  - ‘we make from’\n",
    "  - ‘make from this’\n",
    "  - ‘from this question?’\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80424fbf-7785-4c05-a996-59085778ff41",
   "metadata": {
    "id": "80424fbf-7785-4c05-a996-59085778ff41"
   },
   "source": [
    "Q4: **True or False: Bag of words only counts one occurrence of each word in the text?**\n",
    "- False - Bag of Words counts the total number of times each word appears in the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67094ae3-196f-4845-9a05-b658c55f4012",
   "metadata": {
    "id": "67094ae3-196f-4845-9a05-b658c55f4012"
   },
   "source": [
    "Q5: **What package(s) can we import to help us with removing punctuation?**\n",
    "- import string, re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c2928-7b71-4c23-a737-f1bf68ede1c4",
   "metadata": {
    "id": "be7c2928-7b71-4c23-a737-f1bf68ede1c4"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
