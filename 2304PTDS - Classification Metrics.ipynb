{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7d65ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification Metrics\n",
    "\n",
    "In today's session:\n",
    "\n",
    "- Confusion Matrix\n",
    "- Classification Metrics\n",
    "    - Accuracy\n",
    "    - Recall (+ False Negative Rate)\n",
    "    - Specificity (+ False Positive Rate)\n",
    "    - Precision (+ Negative Predicted Value)\n",
    "    - Balanced Accuracy\n",
    "    - F1 Score\n",
    "- AUC/ROC curve\n",
    "- Classification Report\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe119f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Objectives:**\n",
    "\n",
    "By the end of this session:\n",
    "\n",
    "- Understand how to read a confusion matrix\n",
    "- Given equations, know how to use a confusion matrix to calculate metrics\n",
    "- Describe an AUC/ROC curve\n",
    "- Attempt to create a classification report after fitting a model using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9f554",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1b6d3",
   "metadata": {},
   "source": [
    "Understanding metrics for any machine learning problem is crucial to picking out the best performing model for your use case. We use metrics to not only assess the performance of our one model, but to also compare it to others we create. \n",
    "\n",
    "In regression, we were introduced to metrics such as the MSE and the RMSE. In classification, we need to use slightly different error metrics, as we are predicting labels/categories rather than continuous numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a84b67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's first import some data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e408c05c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af642af1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0   9046    Male  67.0             0              1          Yes   \n",
       "1  51676  Female  61.0             0              0          Yes   \n",
       "2  31112    Male  80.0             0              1          Yes   \n",
       "3  60182  Female  49.0             0              0          Yes   \n",
       "4   1665  Female  79.0             1              0          Yes   \n",
       "\n",
       "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        Private          Urban             228.69  36.6  formerly smoked   \n",
       "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
       "2        Private          Rural             105.92  32.5     never smoked   \n",
       "3        Private          Urban             171.23  34.4           smokes   \n",
       "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data in and view first few entries\n",
    "df = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a81ec3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4861\n",
       "1     249\n",
       "Name: stroke, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d400a8b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# some very simple pre-processing before we get into modelling\n",
    "df = df.dropna()\n",
    "\n",
    "# labels\n",
    "y = df['stroke']\n",
    "\n",
    "# features\n",
    "X = df.drop('stroke', axis=1)\n",
    "\n",
    "# transforming the features\n",
    "X_transformed = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "#now let's do a train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94355143",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# finally we train our very simple model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# and make predictions\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e04eaa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749bc3eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Confusion Matrix\n",
    "\n",
    "![](https://media3.giphy.com/media/eIm624c8nnNbiG0V3g/giphy.gif)\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model. It compares actual and predicted labels of a set of data to create a summary of the model's performance.\n",
    "\n",
    "A confusion matrixs contains four metrics: True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN)\n",
    "\n",
    "![](https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png?w=816)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa802fae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **True Negative**: Predicted Negative, is Negative\n",
    "- **False Positive**: Predicted Positive, is Negative *(Type I error)*\n",
    "- **False Negative**: Predicted Negative, is Positive *(Type II error)*\n",
    "- **True Positive**: Predicted Positive, is Positive\n",
    "\n",
    "\n",
    "Let's put this in the example of COVID-19 testing...\n",
    "- True Negative = Individual tests negative, and they truly do not have COVID\n",
    "- False Positive = Individual tests positive, but they truly do not have COVID\n",
    "- False Negative = ...?\n",
    "- True Positive = ...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6e2455",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[938,   3],\n",
       "       [ 40,   1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a confusion matrix for our model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d76cc2e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0: no stroke</th>\n",
       "      <th>1: stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0: no stroke</th>\n",
       "      <td>938</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1: stroke</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0: no stroke  1: stroke\n",
       "0: no stroke           938          3\n",
       "1: stroke               40          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['0: no stroke', '1: stroke']\n",
    "\n",
    "pd.DataFrame(data = confusion_matrix(y_test, y_pred), index = labels, columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c83adcc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 3 40 1\n"
     ]
    }
   ],
   "source": [
    "# Set up our TN, FP, FN, and TP\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98ac4f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353d31a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Classification Metrics\n",
    "\n",
    "![](https://media.tenor.com/hh6n7Ou_OnUAAAAC/math-hangover.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180b867",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "Ratio of total number of *correctly* predicted instances to total number of instances.\n",
    "\n",
    "$ Accuracy = \\frac{(TP + TN)}{(TP + FP + TN + FN)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0073825",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9562118126272913\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13100a34",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d16e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recall (+ False Negative Rate)\n",
    "\n",
    "Recall (also known as **sensitivity** or **true positive rate**) is the ratio of true positive predictions to the total number of *actual* positive instances in the dataset. \n",
    "\n",
    "--> how well the model is able to identify positive instances out of all the positive instances in the dataset\n",
    "\n",
    "Recall is important in cases where the cost of missing a positive instance is high - e.g. in medical diagnoses: predicting a false negative (saying someone doesn't have a disease when they in fact do) can be costly to the patient.\n",
    "\n",
    "High recall can also lead to a high amount of false positives, however.\n",
    "\n",
    "$ Recall = \\frac{TP}{TP + FN} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28963e21",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**False Negative Rate**\n",
    "\n",
    "Ratio of false negative predictions to total number of actual positive instances in the dataset. High FNR means the model is missing many positive instances. False negatives can be reduced by improving sensitivity/recall, but again this comes at the risk of increasing the number of false positives\n",
    "\n",
    "$FNR = \\frac{FN}{TP + FN} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baaaf400",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.024390243902439025\n",
      "FNR:  0.975609756097561\n"
     ]
    }
   ],
   "source": [
    "recall = (tp) / (tp + fn)\n",
    "fnr = (fn) / (tp + fn)\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"FNR: \", fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288969b9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce2774",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Specificity (+ False Positive Rate)\n",
    "\n",
    "Specificity (aka **True Negative Rate**) is the ratio of true negative predictions to the total number of actual negativeinstances in the dataset.\n",
    "\n",
    "--> how well our model is able to identify negative instances in our dataset\n",
    "\n",
    "Specificity is important in situations where the cost of a false positive is high - e.g. security at an airport: we'd want high specificity to avoid unneccessary extra checks and invasions of privacy, but this can also lead to an increase in false negatives (i.e. more security risks)\n",
    "\n",
    "$Specificity = \\frac{TN}{TN + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2f3a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**False Positive Rate**\n",
    "\n",
    "Ratio of false positive predictions to total number of actual negative instances in the dataset. High FPR indicates the model is incorrectly classifying many negative instances as positive. False positives can be reduced by improving specificity, but again this comes at the risk of increasing the number of false negatives\n",
    "\n",
    "\n",
    "\n",
    "$FPR = \\frac{FP}{TN + FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4102a82d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity:  0.9968119022316685\n",
      "FPR:  0.003188097768331562\n"
     ]
    }
   ],
   "source": [
    "specificity = (tn) / (tn + fp)\n",
    "fpr = (fp) / (tn + fp)\n",
    "\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"FPR: \", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1a357",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9d8a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Precision (+ Negative Predicted Value)\n",
    "\n",
    "Precision (aka **positive predicted value**) is the ratio of true positive predictions to the total number of positve predictions made. High precision indicates the model is making accurate positive predictions (i.e. fewer false positives) - however, it may still be missing many true positive instances.\n",
    "\n",
    "High precision is important when the cost of false positive prediction is high such as in medical diagnoses.\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba38068",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Negative Predicted Value**\n",
    "\n",
    "Ratio of true negative predictions to the total number of actual negative instances in the dataset. Indicates how well our model is identifying negative instances\n",
    "\n",
    "High NPV is important when the cost of false negatives prediction is high such as in medical diagnoses.\n",
    "\n",
    "$NPV = \\frac{TN}{TN + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac449b8c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.25\n",
      "NPV:  0.9591002044989775\n"
     ]
    }
   ],
   "source": [
    "precision = (tp) / (tp + fp)\n",
    "npv = (tn) / (tn + fn)\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"NPV: \", npv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd2f913",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403373bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Balanced Accuracy\n",
    "\n",
    "Average of **sensitivity** and **specificity**. Indicates our model's ability to accurately predict each class label. Particularly useful when dealing with **data imbalance**\n",
    "\n",
    "$Balanced Accuracy = \\frac{Sensitivity + Specificity}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f75d782",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy:  0.5106010730670537\n"
     ]
    }
   ],
   "source": [
    "bal_acc = (recall + specificity) / 2\n",
    "\n",
    "print(\"Balanced Accuracy: \", bal_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c92c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9689a4e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### F1 Score\n",
    "\n",
    "Weighted average of **precision** and **recall** - it is the harmonic mean of these two values and provides a balance between the two. \n",
    "\n",
    "F1 scores fall between 0 and 1, with 0 being the worst and 1 being the best.\n",
    "\n",
    "Indicates how accurately our model is predicting both positive and negative instances, and is particularly useful when the cost of false positives and false negatives is similar\n",
    "\n",
    "$F1 score = 2 * \\frac{precision * recall}{precision + recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ceee7de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.04444444444444444\n"
     ]
    }
   ],
   "source": [
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a7a67",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1c3ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**As always, sklearn has its ways of making our lives much easier. It has built in functions for many of these metrics!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fd6ee71",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c86f916f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "\n",
    "prec = precision_score(y_test, y_pred)\n",
    "\n",
    "rec = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10cc37d8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04444444444444444"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca98fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f467f35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. AUC - ROC Curve\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:494/1*EPmzi0GCgdLstsJb6Q8e-w.png)\n",
    "\n",
    "\n",
    "\n",
    "**ROC Curve**:\n",
    "\n",
    "- Receiver Operating Characteristics. \n",
    "- Graphical representation of how well our classification model classifies! It is a plot of the true positive rate (TPR) against the false positive rate (FPR) at different classification thresholds.\n",
    "- Can help us to identify the optimal classification threshold for the problem at hand\n",
    "- A good model will have a high TPR and a low FPR, indicating that it is accurately predicting positive instances while minimising false positives\n",
    "\n",
    "![](https://dorianbrown.dev/assets/images/logreg/prob_threshold.png)\n",
    "\n",
    "**AUC**:\n",
    "\n",
    "- Area Under the Curve\n",
    "- Represents the area under the ROC curve. It measures the overall performance of the binary classification model. The area will always lie between 0 and 1.\n",
    "- Greater value of AUC = better model performance. \n",
    "- An AUC value of 0.5 means the classifier is doing no better than random chance classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceecc2f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://www.datasciencecentral.com/wp-content/uploads/2021/10/1341805045.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe71fca7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6819159690002851"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find the AUC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb410492",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399593ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Classification Report\n",
    "\n",
    "A table that provides a summary of the performance of our classification model, for each class in the target variable! This is useful for interpreting the performance of our model. In general, a model with high precision, recall and F1-score is considered to be accurate and reliable (but - this does depend on the specific context of your problem at hand, depending on the consequences of FNs and FPs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82e7bd2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Stroke       0.96      1.00      0.98       941\n",
      "      Stroke       0.25      0.02      0.04        41\n",
      "\n",
      "    accuracy                           0.96       982\n",
      "   macro avg       0.60      0.51      0.51       982\n",
      "weighted avg       0.93      0.96      0.94       982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names = ['No Stroke', 'Stroke']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d638ab6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213f9bb",
   "metadata": {},
   "source": [
    "# Let's check our understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f4f0a",
   "metadata": {},
   "source": [
    "1. We are checking whether a dataset of credit card transactions are 'fraud' or 'not fraud'. What would a False Negative be in this situation? *Hint: start by thinking about which is 'positive' and which is 'negative'* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57afe93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d1b6a79",
   "metadata": {},
   "source": [
    "2. Given the following, calculate the **recall**\n",
    "\n",
    "- TP = 73\n",
    "- FP = 4\n",
    "- FN = 11\n",
    "- TN = 39\n",
    "\n",
    "\n",
    "$ Recall = \\frac{TP}{TP + FN} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118389b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aa5ab04",
   "metadata": {},
   "source": [
    "3. True/False: An F1 score of 0 indicates a really well-performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952dc10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4040fcdc",
   "metadata": {},
   "source": [
    "4. What two metrics are on the axes of an ROC curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf5b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2866042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
